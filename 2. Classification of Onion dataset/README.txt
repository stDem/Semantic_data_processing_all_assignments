Text Classification using Word2Vec and FastText EmbeddingsProject Description:This project implements an approach to classify textual data (e.g., news headlines) using embeddings generated by Word2Vec and FastText models. Texts are represented by computing the average of the word embeddings, and various machine learning classifiers are then trained on these features. The models used include Logistic Regression, Random Forest, Linear SVC, KNN, SVC with an RBF kernel, Gradient Boosting, and XGBoost.Main Steps:1. Preprocessing and Text Vectorization:Tokenize the texts.Train the Word2Vec and FastText models.Calculate the average of the word embeddings for each text.2. Classifier Training:Split the data into training and test sets using stratification.Train machine learning models on the generated embeddings.3. Evaluation:Compute metrics such as accuracy, precision, recall, and F1-score.How to Run the Project:1. Prepare a dataset with at least two columns: text (containing the textual data) and label (containing the class labels).2. Install the necessary libraries (e.g., using pip).3. Run the main script which:Generates embeddings using Word2Vec and FastText.Splits the data into training and test sets.Trains a set of classifiers and displays the evaluation metrics for each combination of vectorization method and model.This project demonstrates how modern text vectorization techniques and machine learning models can be effectively used for text classification tasks. High performance metrics may indicate that the embeddings capture the semantic features of the headlines well and that the differences between classes in the dataset are clear.